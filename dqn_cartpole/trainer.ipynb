{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import gymnasium as gym\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-4\n",
    "EPS_END = 0.005\n",
    "EPS_START = 1\n",
    "BATCH_SIZE = 128 # expirement with this more\n",
    "EPS_DECAY = 1000 # does this cause faster or slower update?\n",
    "TAU = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "        \n",
    "    def append(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.memory[i]\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(n_observations, 128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(128, n_actions),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "n_observations = env.observation_space.shape[0]\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions)\n",
    "target_net = DQN(n_observations, n_actions)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "opt = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "criterion = nn.HuberLoss()\n",
    "replay_memory = ReplayMemory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "\n",
    "    eps = EPS_END + (EPS_START - EPS_END) * np.exp(\n",
    "        -1.0 * steps_done / EPS_DECAY\n",
    "    )  # try to look at how eps is updated\n",
    "    steps_done += 1\n",
    "    print(f\"eps: {eps}, steps_done: {steps_done}\")\n",
    "\n",
    "    if random.random() < eps:\n",
    "        torch.tensor([[env.action_space.sample()]], dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():  # attempt without this\n",
    "        return torch.argmax(policy_net(state)).view(\n",
    "            1, 1\n",
    "        )  # why this weird shape? why not get the item directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(replay_memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    transitions = replay_memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(\n",
    "        list(map(lambda state: state is not None, batch.next_state))\n",
    "    )\n",
    "\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    q_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_q_values = torch.zeros(BATCH_SIZE)\n",
    "    next_q_values[non_final_mask] = (\n",
    "        target_net(non_final_next_states).max(1).values.detach()\n",
    "    )\n",
    "\n",
    "    expected_q_values = reward_batch + next_q_values\n",
    "\n",
    "    loss = criterion(q_values, expected_q_values)\n",
    "\n",
    "    opt.zero_grad()\n",
    "\n",
    "    # test\n",
    "    grads = [p.grad for p in policy_net.parameters() if p.grad is not None]\n",
    "    all_grads = torch.cat([g.view(-1) for g in grads])\n",
    "    print(\"Before clipping:\")\n",
    "    print(\"Max grad:\", all_grads.max().item())\n",
    "    print(\"Min grad:\", all_grads.min().item())\n",
    "    print(\"Mean grad:\", all_grads.mean().item())\n",
    "    print(\"Std grad:\", all_grads.std().item())\n",
    "\n",
    "    nn.utils.clip_grad_value_(policy_net.parameters(), 100) # test without it\n",
    "\n",
    "    grads = [p.grad for p in policy_net.parameters() if p.grad is not None]\n",
    "    all_grads = torch.cat([g.view(-1) for g in grads])\n",
    "    print(\"After clipping:\")\n",
    "    print(\"Max grad:\", all_grads.max().item())\n",
    "    print(\"Min grad:\", all_grads.min().item())\n",
    "    print(\"Mean grad:\", all_grads.mean().item())\n",
    "    print(\"Std grad:\", all_grads.std().item())\n",
    "    # end test\n",
    "\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps: 0.8804541122489206, steps_done: 129\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeyadcode/.venv/pytorch_env/lib/python3.12/site-packages/torch/nn/modules/loss.py:1100: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.huber_loss(input, target, reduction=self.reduction, delta=self.delta)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): expected a non-empty list of Tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m replay_memory\u001b[38;5;241m.\u001b[39mappend(state, action, next_state, reward)\n\u001b[1;32m     22\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[0;32m---> 24\u001b[0m \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m target_net_state_dict \u001b[38;5;241m=\u001b[39m target_net\u001b[38;5;241m.\u001b[39mstate_dict()\n\u001b[1;32m     27\u001b[0m policy_net_state_dict \u001b[38;5;241m=\u001b[39m policy_net\u001b[38;5;241m.\u001b[39mstate_dict()\n",
      "Cell \u001b[0;32mIn[76], line 34\u001b[0m, in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m grads \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m policy_net\u001b[38;5;241m.\u001b[39mparameters() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(grads)\n\u001b[0;32m---> 34\u001b[0m all_grads \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBefore clipping:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax grad:\u001b[39m\u001b[38;5;124m\"\u001b[39m, all_grads\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch.cat(): expected a non-empty list of Tensors"
     ]
    }
   ],
   "source": [
    "num_episodes = 1000\n",
    "\n",
    "total_rewards = []\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # (1, 4)\n",
    "\n",
    "    done = False\n",
    "    R = 0\n",
    "    while not done:\n",
    "        action = select_action(state)  # (1, 1)\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "\n",
    "        reward = torch.tensor(reward, dtype=torch.float32).unsqueeze(0)  # (1, 1)\n",
    "\n",
    "        next_state = None if done else torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        replay_memory.append(state, action, next_state, reward)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        optimize_model()\n",
    "\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "\n",
    "        for key in target_net_state_dict:  # inspect how this key looks like\n",
    "            target_net_state_dict[key] = (\n",
    "                TAU * policy_net_state_dict[key]\n",
    "                + (1 - TAU) * target_net_state_dict[key]\n",
    "            )\n",
    "        R += reward\n",
    "    \n",
    "    total_rewards += R\n",
    "    print(R)\n",
    "\n",
    "plt.plot(total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n",
      "hi\n"
     ]
    }
   ],
   "source": [
    "for key in target_net.state_dict():\n",
    "    print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
