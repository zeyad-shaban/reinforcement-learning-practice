{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-05 07:58:48.057872: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-05 07:58:48.059679: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-05 07:58:48.068542: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-05 07:58:48.103284: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738735128.165780   24610 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738735128.185897   24610 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-05 07:58:48.232083: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import gymnasium as gym\n",
    "from dataclasses import dataclass\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params\n",
    "BATCH_SIZE = 128\n",
    "PERCENTILE = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(1, 100),\n",
    "    nn.ReLU(),\n",
    "\n",
    "    nn.Linear(100, 50),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    nn.Linear(50, 25),\n",
    "    nn.ReLU(),\n",
    "    \n",
    "    nn.Linear(25, 4),\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Episode:\n",
    "    observations: list[list[float]]\n",
    "    actions: list[int]\n",
    "    reward: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeyadcode/.venv/pytorch_env/lib/python3.12/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/zeyadcode/workspace/Sandbox/reinforcement/frozen_lake_killer/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 reward mean: 97.0 percentile: 82.4 best: 168.0\n",
      "epoch 1 reward mean: 102.25 percentile: 109.19999999999999 best: 145.0\n",
      "epoch 2 reward mean: 89.27272727272727 percentile: 89.0 best: 140.0\n",
      "epoch 3 reward mean: 93.5 percentile: 101.7 best: 114.0\n",
      "epoch 4 reward mean: 78.8 percentile: 82.2 best: 85.0\n",
      "epoch 5 reward mean: 98.5 percentile: 103.3 best: 141.0\n",
      "epoch 6 reward mean: 110.54545454545455 percentile: 118.0 best: 149.0\n",
      "epoch 7 reward mean: 97.8 percentile: 99.2 best: 125.0\n",
      "epoch 8 reward mean: 105.77777777777777 percentile: 125.2 best: 144.0\n",
      "epoch 9 reward mean: 100.42857142857143 percentile: 107.0 best: 168.0\n",
      "epoch 10 reward mean: 103.25 percentile: 106.0 best: 140.0\n",
      "epoch 11 reward mean: 122.6 percentile: 130.5 best: 203.0\n",
      "epoch 12 reward mean: 101.5 percentile: 113.2 best: 130.0\n",
      "epoch 13 reward mean: 128.125 percentile: 130.0 best: 225.0\n",
      "epoch 14 reward mean: 96.75 percentile: 87.3 best: 154.0\n",
      "epoch 15 reward mean: 109.26666666666667 percentile: 122.39999999999999 best: 168.0\n",
      "epoch 16 reward mean: 108.33333333333333 percentile: 114.8 best: 155.0\n",
      "epoch 17 reward mean: 89.125 percentile: 92.3 best: 118.0\n",
      "epoch 18 reward mean: 89.66666666666667 percentile: 95.0 best: 100.0\n",
      "epoch 19 reward mean: 128.55555555555554 percentile: 120.0 best: 275.0\n",
      "epoch 20 reward mean: 103.6 percentile: 114.2 best: 136.0\n",
      "epoch 21 reward mean: 94.6 percentile: 100.2 best: 137.0\n",
      "epoch 22 reward mean: 93.55555555555556 percentile: 105.8 best: 111.0\n",
      "epoch 23 reward mean: 96.4 percentile: 83.6 best: 170.0\n",
      "epoch 24 reward mean: 127.33333333333333 percentile: 148.5 best: 204.0\n",
      "epoch 25 reward mean: 89.55555555555556 percentile: 87.6 best: 134.0\n",
      "epoch 26 reward mean: 124.0 percentile: 138.5 best: 209.0\n",
      "epoch 27 reward mean: 111.66666666666667 percentile: 119.79999999999998 best: 172.0\n",
      "epoch 28 reward mean: 126.5 percentile: 133.1 best: 247.0\n",
      "epoch 29 reward mean: 85.36363636363636 percentile: 93.0 best: 116.0\n",
      "epoch 30 reward mean: 92.16666666666667 percentile: 90.0 best: 151.0\n",
      "epoch 31 reward mean: 97.0 percentile: 103.79999999999998 best: 121.0\n",
      "epoch 32 reward mean: 109.77777777777777 percentile: 133.2 best: 141.0\n",
      "epoch 33 reward mean: 94.75 percentile: 101.5 best: 106.0\n",
      "epoch 34 reward mean: 105.83333333333333 percentile: 111.69999999999999 best: 174.0\n",
      "epoch 35 reward mean: 99.77777777777777 percentile: 111.39999999999999 best: 126.0\n",
      "epoch 36 reward mean: 104.33333333333333 percentile: 114.6 best: 136.0\n",
      "epoch 37 reward mean: 91.0 percentile: 100.0 best: 113.0\n",
      "epoch 38 reward mean: 99.57142857142857 percentile: 108.19999999999999 best: 140.0\n",
      "epoch 39 reward mean: 99.44444444444444 percentile: 105.39999999999999 best: 127.0\n",
      "epoch 40 reward mean: 106.8 percentile: 115.5 best: 240.0\n",
      "epoch 41 reward mean: 98.16666666666667 percentile: 113.5 best: 125.0\n",
      "epoch 42 reward mean: 90.57142857142857 percentile: 96.0 best: 111.0\n",
      "epoch 43 reward mean: 91.92857142857143 percentile: 92.5 best: 143.0\n",
      "epoch 44 reward mean: 105.0 percentile: 107.39999999999999 best: 209.0\n",
      "epoch 45 reward mean: 121.16666666666667 percentile: 132.0 best: 161.0\n",
      "epoch 46 reward mean: 86.375 percentile: 82.9 best: 131.0\n",
      "epoch 47 reward mean: 114.14285714285714 percentile: 129.39999999999998 best: 159.0\n",
      "epoch 48 reward mean: 111.0 percentile: 126.19999999999999 best: 154.0\n",
      "epoch 49 reward mean: 100.0 percentile: 96.6 best: 173.0\n",
      "epoch 50 reward mean: 76.0 percentile: 76.8 best: 78.0\n",
      "epoch 51 reward mean: 127.625 percentile: 110.6 best: 236.0\n",
      "epoch 52 reward mean: 95.8 percentile: 109.2 best: 115.0\n",
      "epoch 53 reward mean: 117.16666666666667 percentile: 135.0 best: 162.0\n",
      "epoch 54 reward mean: 103.46666666666667 percentile: 100.19999999999999 best: 172.0\n",
      "epoch 55 reward mean: 97.625 percentile: 114.8 best: 128.0\n",
      "epoch 56 reward mean: 99.42857142857143 percentile: 108.8 best: 119.0\n",
      "epoch 57 reward mean: 102.07142857142857 percentile: 94.3 best: 235.0\n",
      "epoch 58 reward mean: 103.66666666666667 percentile: 105.5 best: 183.0\n",
      "epoch 59 reward mean: 103.0 percentile: 104.79999999999998 best: 168.0\n",
      "epoch 60 reward mean: 126.3 percentile: 118.6 best: 322.0\n",
      "epoch 61 reward mean: 101.625 percentile: 112.3 best: 157.0\n",
      "epoch 62 reward mean: 137.25 percentile: 160.79999999999998 best: 201.0\n",
      "epoch 63 reward mean: 103.5 percentile: 98.79999999999997 best: 169.0\n",
      "epoch 64 reward mean: 103.75 percentile: 109.29999999999998 best: 148.0\n",
      "epoch 65 reward mean: 94.4 percentile: 103.0 best: 118.0\n",
      "epoch 66 reward mean: 123.125 percentile: 122.39999999999999 best: 244.0\n",
      "epoch 67 reward mean: 100.0 percentile: 106.4 best: 116.0\n",
      "epoch 68 reward mean: 86.5 percentile: 94.5 best: 110.0\n",
      "epoch 69 reward mean: 95.77777777777777 percentile: 101.6 best: 166.0\n",
      "epoch 70 reward mean: 105.5 percentile: 108.69999999999999 best: 124.0\n",
      "epoch 71 reward mean: 106.875 percentile: 106.0 best: 146.0\n",
      "epoch 72 reward mean: 100.11111111111111 percentile: 107.6 best: 143.0\n",
      "epoch 73 reward mean: 96.66666666666667 percentile: 101.2 best: 135.0\n",
      "epoch 74 reward mean: 102.72727272727273 percentile: 96.0 best: 201.0\n",
      "epoch 75 reward mean: 114.75 percentile: 106.89999999999996 best: 196.0\n",
      "epoch 76 reward mean: 96.2 percentile: 104.3 best: 151.0\n",
      "epoch 77 reward mean: 218.0 percentile: 218.0 best: 218.0\n",
      "epoch 78 reward mean: 104.0 percentile: 99.4 best: 174.0\n",
      "epoch 79 reward mean: 142.25 percentile: 161.0 best: 197.0\n",
      "epoch 80 reward mean: 103.83333333333333 percentile: 104.5 best: 159.0\n",
      "epoch 81 reward mean: 93.0 percentile: 101.6 best: 110.0\n",
      "epoch 82 reward mean: 102.75 percentile: 124.0 best: 133.0\n",
      "epoch 83 reward mean: 112.16666666666667 percentile: 124.5 best: 158.0\n",
      "epoch 84 reward mean: 96.5 percentile: 104.3 best: 116.0\n",
      "epoch 85 reward mean: 97.5 percentile: 97.0 best: 138.0\n",
      "epoch 86 reward mean: 93.33333333333333 percentile: 98.0 best: 112.0\n",
      "epoch 87 reward mean: 106.66666666666667 percentile: 117.0 best: 169.0\n",
      "epoch 88 reward mean: 100.22222222222223 percentile: 107.8 best: 119.0\n",
      "epoch 89 reward mean: 111.33333333333333 percentile: 124.6 best: 176.0\n",
      "epoch 90 reward mean: 100.57142857142857 percentile: 109.6 best: 123.0\n",
      "epoch 91 reward mean: 86.6923076923077 percentile: 87.4 best: 123.0\n",
      "epoch 92 reward mean: 89.5 percentile: 96.0 best: 109.0\n",
      "epoch 93 reward mean: 80.5 percentile: 81.3 best: 93.0\n",
      "epoch 94 reward mean: 119.72222222222223 percentile: 133.79999999999998 best: 240.0\n",
      "epoch 95 reward mean: 140.75 percentile: 176.49999999999997 best: 232.0\n",
      "epoch 96 reward mean: 90.5 percentile: 103.5 best: 107.0\n",
      "epoch 97 reward mean: 99.16666666666667 percentile: 103.0 best: 133.0\n",
      "epoch 98 reward mean: 107.5 percentile: 130.0 best: 138.0\n",
      "epoch 99 reward mean: 76.0 percentile: 76.0 best: 76.0\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "epochs = 100\n",
    "\n",
    "env = gym.make('FrozenLake-v1', render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, './videos')\n",
    "action_space = list(range(env.action_space.n))\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    best_of_the_best = []\n",
    "\n",
    "    # episodes loop\n",
    "    for batch in range(BATCH_SIZE):\n",
    "        obs, _ = env.reset()\n",
    "        episode = Episode(observations=[], actions=[], reward=0)\n",
    "\n",
    "        # game loop\n",
    "        while True:\n",
    "            obs_t = torch.tensor([obs], dtype=torch.float32, device=device)\n",
    "            action_pred = model(obs_t)\n",
    "            action_p_dist = torch.softmax(action_pred, dim=0)\n",
    "            action = np.random.choice(\n",
    "                action_space, p=action_p_dist.detach().cpu().numpy()\n",
    "            )\n",
    "\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            if terminated or truncated:\n",
    "                if terminated:\n",
    "                    episode.reward -= 10\n",
    "                break\n",
    "\n",
    "            episode.observations.append(obs)\n",
    "            episode.actions.append(action)\n",
    "            episode.reward += reward * 100\n",
    "            episode.reward -= 1\n",
    "            episode.reward += obs\n",
    "\n",
    "            obs = next_obs\n",
    "\n",
    "        if episode.reward > 70:\n",
    "            best_of_the_best.append(episode)\n",
    "\n",
    "    # pick only elite episodes\n",
    "    if len(best_of_the_best) == 0:\n",
    "        continue\n",
    "    rewards = np.array([episode.reward for episode in best_of_the_best])\n",
    "    percentile = np.percentile(rewards, PERCENTILE)\n",
    "    reward_mean = np.mean(rewards)\n",
    "    best_max = np.max(rewards)\n",
    "\n",
    "    writer.add_scalar('reward/mean', reward_mean, epoch)\n",
    "    writer.add_scalar('reward/percentile', percentile, epoch)\n",
    "    writer.add_scalar('reward/best', best_max, epoch)\n",
    "\n",
    "    opt.zero_grad()\n",
    "\n",
    "    for episode in best_of_the_best:\n",
    "        if episode.reward < percentile:\n",
    "            continue\n",
    "\n",
    "        obs_t = torch.tensor(episode.observations, dtype=torch.float32, device=device).view(-1, 1) # shape (n, 1)\n",
    "        action_target = torch.tensor(episode.actions, dtype=torch.long, device=device) # shape (n)\n",
    "        action_pred = model(obs_t) # shape (n, 4)\n",
    "        loss = criterion(action_pred, action_target)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "    opt.step()\n",
    "\n",
    "    print(f'epoch {epoch} reward mean: {reward_mean} percentile: {percentile} best: {best_max}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './weights.pth')\n",
    "torch.save(model, './complete_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
