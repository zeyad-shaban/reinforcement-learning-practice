{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 04:14:24.393710: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-02 04:14:24.403807: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740881664.417603  530957 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740881664.421660  530957 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-02 04:14:24.436045: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils import write_grad_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Actor2Critic(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(in_channels, 128),\n",
    "            nn.ReLU(),\n",
    "            #\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            #\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.actor_head = nn.Sequential(\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, out_channels),\n",
    "        )\n",
    "\n",
    "        self.critic_head = nn.Sequential(\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        y = self.backbone(x)\n",
    "\n",
    "        actor = self.actor_head(y)\n",
    "        critic = self.critic_head(y)\n",
    "\n",
    "        return actor, critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "GAMMA = 0.99\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 16\n",
    "START_BETA = 1\n",
    "BETA_END = 0.0001\n",
    "BETA_DECAY = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "n_obserations = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "net = Actor2Critic(n_obserations, n_actions)\n",
    "opt = optim.Adam(net.parameters(), LR)\n",
    "critic_criterion = nn.MSELoss()\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, R: 11.0, beta: 0.9990001\n",
      "episode: 1, R: 14.0, beta: 0.99760024\n",
      "episode: 2, R: 17.0, beta: 0.99590041\n",
      "episode: 3, R: 10.0, beta: 0.99490051\n",
      "episode: 4, R: 12.0, beta: 0.99370063\n",
      "episode: 5, R: 28.0, beta: 0.99090091\n",
      "episode: 6, R: 33.0, beta: 0.98760124\n",
      "episode: 7, R: 18.0, beta: 0.98580142\n",
      "episode: 8, R: 23.0, beta: 0.98350165\n",
      "episode: 9, R: 16.0, beta: 0.98190181\n",
      "episode: 10, R: 29.0, beta: 0.9790021\n",
      "episode: 11, R: 14.0, beta: 0.9776022400000001\n",
      "episode: 12, R: 10.0, beta: 0.97660234\n",
      "episode: 13, R: 16.0, beta: 0.9750025\n",
      "episode: 14, R: 9.0, beta: 0.97410259\n",
      "episode: 15, R: 16.0, beta: 0.9725027500000001\n",
      "episode: 16, R: 25.0, beta: 0.970003\n",
      "episode: 17, R: 12.0, beta: 0.96880312\n",
      "episode: 18, R: 19.0, beta: 0.96690331\n",
      "episode: 19, R: 17.0, beta: 0.96520348\n",
      "episode: 20, R: 12.0, beta: 0.9640036\n",
      "episode: 21, R: 19.0, beta: 0.9621037899999999\n",
      "episode: 22, R: 15.0, beta: 0.96060394\n",
      "episode: 23, R: 12.0, beta: 0.95940406\n",
      "episode: 24, R: 12.0, beta: 0.9582041800000001\n",
      "episode: 25, R: 29.0, beta: 0.9553044700000001\n",
      "episode: 26, R: 15.0, beta: 0.95380462\n",
      "episode: 27, R: 26.0, beta: 0.95120488\n",
      "episode: 28, R: 23.0, beta: 0.94890511\n",
      "episode: 29, R: 24.0, beta: 0.94650535\n",
      "episode: 30, R: 16.0, beta: 0.94490551\n",
      "episode: 31, R: 29.0, beta: 0.9420057999999999\n",
      "episode: 32, R: 17.0, beta: 0.94030597\n",
      "episode: 33, R: 15.0, beta: 0.93880612\n",
      "episode: 34, R: 18.0, beta: 0.9370063000000001\n",
      "episode: 35, R: 56.0, beta: 0.9314068600000001\n",
      "episode: 36, R: 44.0, beta: 0.9270073000000001\n",
      "episode: 37, R: 21.0, beta: 0.92490751\n",
      "episode: 38, R: 25.0, beta: 0.92240776\n",
      "episode: 39, R: 18.0, beta: 0.92060794\n",
      "episode: 40, R: 23.0, beta: 0.91830817\n",
      "episode: 41, R: 23.0, beta: 0.9160084000000001\n",
      "episode: 42, R: 26.0, beta: 0.91340866\n",
      "episode: 43, R: 22.0, beta: 0.91120888\n",
      "episode: 44, R: 16.0, beta: 0.90960904\n",
      "episode: 45, R: 20.0, beta: 0.90760924\n",
      "episode: 46, R: 17.0, beta: 0.90590941\n",
      "episode: 47, R: 20.0, beta: 0.90390961\n",
      "episode: 48, R: 17.0, beta: 0.90220978\n",
      "episode: 49, R: 37.0, beta: 0.89851015\n",
      "episode: 50, R: 18.0, beta: 0.89671033\n",
      "episode: 51, R: 17.0, beta: 0.8950105\n",
      "episode: 52, R: 38.0, beta: 0.89121088\n",
      "episode: 53, R: 11.0, beta: 0.89011099\n",
      "episode: 54, R: 22.0, beta: 0.8879112100000001\n",
      "episode: 55, R: 24.0, beta: 0.88551145\n",
      "episode: 56, R: 15.0, beta: 0.8840116\n",
      "episode: 57, R: 40.0, beta: 0.880012\n",
      "episode: 58, R: 24.0, beta: 0.87761224\n",
      "episode: 59, R: 23.0, beta: 0.87531247\n",
      "episode: 60, R: 26.0, beta: 0.87271273\n",
      "episode: 61, R: 29.0, beta: 0.86981302\n",
      "episode: 62, R: 17.0, beta: 0.86811319\n",
      "episode: 63, R: 18.0, beta: 0.86631337\n",
      "episode: 64, R: 13.0, beta: 0.8650135\n",
      "episode: 65, R: 11.0, beta: 0.86391361\n",
      "episode: 66, R: 17.0, beta: 0.8622137799999999\n",
      "episode: 67, R: 16.0, beta: 0.86061394\n",
      "episode: 68, R: 18.0, beta: 0.85881412\n",
      "episode: 69, R: 9.0, beta: 0.85791421\n",
      "episode: 70, R: 20.0, beta: 0.85591441\n",
      "episode: 71, R: 14.0, beta: 0.85451455\n",
      "episode: 72, R: 12.0, beta: 0.8533146699999999\n",
      "episode: 73, R: 37.0, beta: 0.84961504\n",
      "episode: 74, R: 21.0, beta: 0.84751525\n",
      "episode: 75, R: 25.0, beta: 0.8450154999999999\n",
      "episode: 76, R: 28.0, beta: 0.8422157800000001\n",
      "episode: 77, R: 39.0, beta: 0.83831617\n",
      "episode: 78, R: 43.0, beta: 0.8340166\n",
      "episode: 79, R: 14.0, beta: 0.83261674\n",
      "episode: 80, R: 31.0, beta: 0.82951705\n",
      "episode: 81, R: 13.0, beta: 0.82821718\n",
      "episode: 82, R: 20.0, beta: 0.82621738\n",
      "episode: 83, R: 33.0, beta: 0.8229177099999999\n",
      "episode: 84, R: 14.0, beta: 0.82151785\n",
      "episode: 85, R: 46.0, beta: 0.8169183099999999\n",
      "episode: 86, R: 14.0, beta: 0.81551845\n",
      "episode: 87, R: 13.0, beta: 0.81421858\n",
      "episode: 88, R: 31.0, beta: 0.8111188899999999\n",
      "episode: 89, R: 11.0, beta: 0.810019\n",
      "episode: 90, R: 10.0, beta: 0.8090191\n",
      "episode: 91, R: 22.0, beta: 0.80681932\n",
      "episode: 92, R: 13.0, beta: 0.80551945\n",
      "episode: 93, R: 15.0, beta: 0.8040196000000001\n",
      "episode: 94, R: 12.0, beta: 0.8028197199999999\n",
      "episode: 95, R: 21.0, beta: 0.80071993\n",
      "episode: 96, R: 13.0, beta: 0.79942006\n",
      "episode: 97, R: 41.0, beta: 0.7953204700000001\n",
      "episode: 98, R: 13.0, beta: 0.7940206000000001\n",
      "episode: 99, R: 18.0, beta: 0.79222078\n",
      "episode: 100, R: 22.0, beta: 0.7900210000000001\n",
      "episode: 101, R: 27.0, beta: 0.78732127\n",
      "episode: 102, R: 17.0, beta: 0.78562144\n",
      "episode: 103, R: 19.0, beta: 0.7837216300000001\n",
      "episode: 104, R: 12.0, beta: 0.78252175\n",
      "episode: 105, R: 28.0, beta: 0.7797220300000001\n",
      "episode: 106, R: 36.0, beta: 0.77612239\n",
      "episode: 107, R: 31.0, beta: 0.7730227000000001\n",
      "episode: 108, R: 36.0, beta: 0.7694230599999999\n",
      "episode: 109, R: 11.0, beta: 0.7683231699999999\n",
      "episode: 110, R: 19.0, beta: 0.76642336\n",
      "episode: 111, R: 20.0, beta: 0.76442356\n",
      "episode: 112, R: 17.0, beta: 0.7627237299999999\n",
      "episode: 113, R: 26.0, beta: 0.76012399\n",
      "episode: 114, R: 13.0, beta: 0.75882412\n",
      "episode: 115, R: 19.0, beta: 0.75692431\n",
      "episode: 116, R: 17.0, beta: 0.75522448\n",
      "episode: 117, R: 24.0, beta: 0.75282472\n",
      "episode: 118, R: 22.0, beta: 0.7506249399999999\n",
      "episode: 119, R: 13.0, beta: 0.74932507\n",
      "episode: 120, R: 14.0, beta: 0.74792521\n",
      "episode: 121, R: 18.0, beta: 0.74612539\n",
      "episode: 122, R: 21.0, beta: 0.7440256\n",
      "episode: 123, R: 16.0, beta: 0.7424257599999999\n",
      "episode: 124, R: 31.0, beta: 0.7393260700000001\n",
      "episode: 125, R: 21.0, beta: 0.7372262800000001\n",
      "episode: 126, R: 16.0, beta: 0.73562644\n",
      "episode: 127, R: 28.0, beta: 0.73282672\n",
      "episode: 128, R: 16.0, beta: 0.73122688\n",
      "episode: 129, R: 14.0, beta: 0.72982702\n",
      "episode: 130, R: 10.0, beta: 0.72882712\n",
      "episode: 131, R: 55.0, beta: 0.7233276700000001\n",
      "episode: 132, R: 11.0, beta: 0.72222778\n",
      "episode: 133, R: 26.0, beta: 0.71962804\n",
      "episode: 134, R: 14.0, beta: 0.71822818\n",
      "episode: 135, R: 15.0, beta: 0.71672833\n",
      "episode: 136, R: 25.0, beta: 0.7142285799999999\n",
      "episode: 137, R: 15.0, beta: 0.71272873\n",
      "episode: 138, R: 15.0, beta: 0.7112288800000001\n",
      "episode: 139, R: 91.0, beta: 0.70212979\n",
      "episode: 140, R: 21.0, beta: 0.7000299999999999\n",
      "episode: 141, R: 13.0, beta: 0.69873013\n",
      "episode: 142, R: 42.0, beta: 0.69453055\n",
      "episode: 143, R: 51.0, beta: 0.68943106\n",
      "episode: 144, R: 26.0, beta: 0.68683132\n",
      "episode: 145, R: 20.0, beta: 0.68483152\n",
      "episode: 146, R: 30.0, beta: 0.68183182\n",
      "episode: 147, R: 12.0, beta: 0.6806319399999999\n",
      "episode: 148, R: 55.0, beta: 0.6751324900000001\n",
      "episode: 149, R: 41.0, beta: 0.6710329\n",
      "episode: 150, R: 29.0, beta: 0.66813319\n",
      "episode: 151, R: 11.0, beta: 0.6670333\n",
      "episode: 152, R: 21.0, beta: 0.66493351\n",
      "episode: 153, R: 35.0, beta: 0.6614338599999999\n",
      "episode: 154, R: 11.0, beta: 0.66033397\n",
      "episode: 155, R: 12.0, beta: 0.65913409\n",
      "episode: 156, R: 19.0, beta: 0.65723428\n",
      "episode: 157, R: 24.0, beta: 0.6548345200000001\n",
      "episode: 158, R: 30.0, beta: 0.6518348199999999\n",
      "episode: 159, R: 21.0, beta: 0.64973503\n",
      "episode: 160, R: 19.0, beta: 0.6478352199999999\n",
      "episode: 161, R: 20.0, beta: 0.6458354199999999\n",
      "episode: 162, R: 38.0, beta: 0.6420358\n",
      "episode: 163, R: 18.0, beta: 0.64023598\n",
      "episode: 164, R: 15.0, beta: 0.6387361300000001\n",
      "episode: 165, R: 16.0, beta: 0.63713629\n",
      "episode: 166, R: 16.0, beta: 0.6355364499999999\n",
      "episode: 167, R: 37.0, beta: 0.63183682\n",
      "episode: 168, R: 14.0, beta: 0.63043696\n",
      "episode: 169, R: 16.0, beta: 0.62883712\n",
      "episode: 170, R: 23.0, beta: 0.6265373500000001\n",
      "episode: 171, R: 11.0, beta: 0.62543746\n",
      "episode: 172, R: 16.0, beta: 0.62383762\n",
      "episode: 173, R: 51.0, beta: 0.61873813\n",
      "episode: 174, R: 11.0, beta: 0.6176382399999999\n",
      "episode: 175, R: 14.0, beta: 0.6162383800000001\n",
      "episode: 176, R: 9.0, beta: 0.61533847\n",
      "episode: 177, R: 14.0, beta: 0.61393861\n",
      "episode: 178, R: 17.0, beta: 0.6122387800000001\n",
      "episode: 179, R: 59.0, beta: 0.60633937\n",
      "episode: 180, R: 11.0, beta: 0.6052394799999999\n",
      "episode: 181, R: 25.0, beta: 0.60273973\n",
      "episode: 182, R: 20.0, beta: 0.60073993\n",
      "episode: 183, R: 18.0, beta: 0.59894011\n",
      "episode: 184, R: 33.0, beta: 0.59564044\n",
      "episode: 185, R: 62.0, beta: 0.58944106\n",
      "episode: 186, R: 16.0, beta: 0.5878412199999999\n",
      "episode: 187, R: 23.0, beta: 0.58554145\n",
      "episode: 188, R: 15.0, beta: 0.5840416\n",
      "episode: 189, R: 19.0, beta: 0.5821417900000001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m BATCH_SIZE \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     38\u001b[0m     opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 40\u001b[0m     \u001b[43m(\u001b[49m\u001b[43mtotal_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     write_grad_info(net\u001b[38;5;241m.\u001b[39mparameters(), writer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m'\u001b[39m, step, is_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     43\u001b[0m     writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal/loss\u001b[39m\u001b[38;5;124m'\u001b[39m, total_loss\u001b[38;5;241m.\u001b[39mitem(), step)\n",
      "File \u001b[0;32m~/.venv/pytorch_env/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/pytorch_env/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/pytorch_env/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episodes = 10000\n",
    "step = 0\n",
    "beta = START_BETA\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    state = torch.tensor(state)\n",
    "\n",
    "    total_loss = torch.tensor(0.0)\n",
    "\n",
    "    R = 0\n",
    "\n",
    "    while True:\n",
    "        beta = BETA_END + (START_BETA - BETA_END) * max((1 - step / BETA_DECAY), 0)\n",
    "        step += 1\n",
    "\n",
    "        action_dist, value = net(state)\n",
    "        categorical = Categorical(logits=action_dist)\n",
    "\n",
    "        action = categorical.sample()\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        next_state = torch.tensor(next_state)\n",
    "        R += reward\n",
    "\n",
    "\n",
    "        next_value = net(next_state)[1] * (truncated or terminated) # zero when there is no next state\n",
    "\n",
    "        td_target = reward + GAMMA * next_value\n",
    "\n",
    "        advantages = (td_target - value).detach() # (value - td_target).detach()\n",
    "        policy_loss = -(advantages * torch.log(categorical.probs[action.item()])).mean()\n",
    "        value_loss = critic_criterion(value, td_target)\n",
    "        entropy_loss = -beta * categorical.entropy()\n",
    "\n",
    "        total_loss += policy_loss + value_loss + entropy_loss + entropy_loss\n",
    "\n",
    "        if step % BATCH_SIZE == 0:\n",
    "            opt.zero_grad()\n",
    "\n",
    "            (total_loss / BATCH_SIZE).backward()\n",
    "\n",
    "            write_grad_info(net.parameters(), writer, 'total', step, is_grad=False)\n",
    "            writer.add_scalar('total/loss', total_loss.item(), step)\n",
    "\n",
    "            opt.step()\n",
    "\n",
    "            total_loss = torch.tensor(0.0)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "    writer.add_scalar('reward', R, episode)\n",
    "    print(f'episode: {episode}, R: {R}, beta: {beta}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), './a2c.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
